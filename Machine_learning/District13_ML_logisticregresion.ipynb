{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c489dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing possible libraries and dependencies\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from path import Path\n",
    "from config import db_password\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import inspect\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60a37d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating string to our Database, engine and calling in dataset\n",
    "db_string = f\"postgresql://postgres:{db_password}@127.0.0.1:5432/Arizona_Elections\"\n",
    "engine = create_engine(db_string)\n",
    "df_voters = pd.read_sql('SELECT * from machinelearning', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2450cbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_voters.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a54a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop all columns contained unecesarry features or null nan \n",
    "df_voters.drop(columns=['Voter Score','voter_id','Turnout Score','Kids in HH','Liberal Ideology'], inplace=True)\n",
    "df_voters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc0ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target column values to low_chance and high_chance based on values\n",
    "\n",
    "x = {'False':'Low_Chance'}\n",
    "df_voters = df_voters.replace(x)\n",
    "\n",
    "x = dict.fromkeys(['True'],'High_Chance')\n",
    "df_voters = df_voters.replace(x)\n",
    "\n",
    "df_voters.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df_voters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121391e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of our conditions\n",
    "conditions = [\n",
    "    (df_voters['Age'] >= 18) & (df_voters['Age'] <= 24),\n",
    "    (df_voters['Age'] >= 25) & (df_voters['Age'] <= 34),\n",
    "    (df_voters['Age'] >= 35) & (df_voters['Age'] <=44),\n",
    "    (df_voters['Age'] >= 45) & (df_voters['Age'] <=54),\n",
    "    (df_voters['Age'] >= 55) & (df_voters['Age'] <=64),\n",
    "    (df_voters['Age'] >= 65),\n",
    "    ]\n",
    "\n",
    "# Create of values we want assigned to the conditions\n",
    "values = ['1', '2', '3','4','5','6']\n",
    "\n",
    "# Create a new column with np.select to assign values to it using our lists as arguments\n",
    "df_voters['Age'] = np.select(conditions, values)\n",
    "\n",
    "# Display updated DataFrame\n",
    "df_voters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dfea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seeing if age code is working properly by adding unique values\n",
    "print(df_voters['Age'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c62ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting label columns from text to numerical data as model only works with numerical data\n",
    "\n",
    "X = pd.get_dummies(df_voters, columns=[\"Party\",\"Sex\",\"Ethnicity\",'Zip']).drop(\"Swing Voter\", axis=1)\n",
    "\n",
    "# Create our target\n",
    "\n",
    "y = df_voters[\"Swing Voter\"]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccef305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verifying out target was selected correclty\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f1379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our training sample and testing sample \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    random_state=1, \n",
    "                                                    stratify=y)\n",
    "# Check balances\n",
    "print(Counter(y_train))\n",
    "print(Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7fbcfa",
   "metadata": {},
   "source": [
    "## undersampling using logistic regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5010240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking our resample counters\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "ros = RandomUnderSampler(random_state=1)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "Counter(y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba9b6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression being process\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='lbfgs', random_state=1)\n",
    "model.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a839c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking out our matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf7180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#viewing accuracy scores\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "balanced_accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f68e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the classification report to see our scores\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "print(classification_report_imbalanced(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073ab798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix.\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, index=[\"Actual High_chane\", \"Actual low_Chance\"], columns=[\"Predicted high_Chance\", \"Predicted low_Chance\"])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e778b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#another way to view our results\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "pd.DataFrame({\"Prediction\": predictions, \"Actual\": y_test}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd543d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
